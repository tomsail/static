{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mesh renumbering "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why renumbering ?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Essentially, renumbering can be important in order to optimize the post-processing of space/time datasets. \n",
    "\n",
    "By slicing the Dataset along the the node index, we can get a chunk of data that is: \n",
    " * coherent spatially to certain region\n",
    " * that is much smaller than the whole dataset\n",
    " * that facilitates multi processing \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "you need both `xarray-selafin` and `thalassa` for this notebook to work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install xarray-selafin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr \n",
    "import thalassa\n",
    "from thalassa import api\n",
    "import holoviews as hv\n",
    "hv.extension(\"bokeh\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we load a 7km mesh of the global ocean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = thalassa.open_dataset(\"meshes/global-v1.2.slf\")\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import hvplot.pandas\n",
    "import numpy as np\n",
    "def plot_mesh(x, y): \n",
    "    df = pd.DataFrame({'x': x, 'y': y, 'id': np.arange(len(x))})\n",
    "    im = df.hvplot.points(x='x', y='y', c='id',s=3)\n",
    "    return im\n",
    "\n",
    "x, y = ds.lon.values, ds.lat.values\n",
    "plot_mesh(x,y).opts(width = 1200, height = 600,cmap='tab20c')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we see that the mesh indeice is not coherent spatially. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "to reorder/ renumber, we will first load a simplified version of the [world maritime borders](https://www.naturalearthdata.com/downloads/10m-physical-vectors/10m-physical-labels/) avaiable at this [gist](https://gist.github.com/tomsail/2fc6c0d9544f6354f9822576fb58b4f7): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "world_oceans = gpd.read_file(\"https://gist.githubusercontent.com/tomsail/2fc6c0d9544f6354f9822576fb58b4f7/raw/5864569a2f410b621ee07e92b782f21a8fbe4e6c/world_oceans.json\")\n",
    "world_oceans.hvplot(color = 'name', width = 1500, height = 900, cmap='tab20c')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the problem with this dataset is that it only covers the oceans, but not the continents. \n",
    "So depending on your meshing, some point might fall on land.\n",
    "\n",
    "We'll remediate to this problem by using this [methodology](https://gis.stackexchange.com/questions/175599/buffer-neighbouring-polygons-without-overlap-using-qgis): \n",
    "1) Extract vertices from polygons, ensuring a unique field is kept as an attribute\n",
    "2) Create a voronoi from these points\n",
    "3) Buffer the original polygons by the required amount -- **not needed here**\n",
    "4) Subtract the buffer polygons from the voronoi\n",
    "5) Recombine/dissolve the remaining voronoi polygons on the unique attribute field\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "from shapely.ops import unary_union\n",
    "import shapely\n",
    "from scipy.spatial import Voronoi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_land_poly(gdf: gpd.GeoDataFrame):\n",
    "    # 1. Extract vertices from polygons, ensuring a unique field is kept as an attribute\n",
    "    points = []\n",
    "    attributes = []\n",
    "    for index, row in gdf.iterrows():\n",
    "        poly = row['geometry']\n",
    "        for geom in poly.geoms:\n",
    "            for coord in geom.exterior.coords:\n",
    "                points.append(shapely.Point(coord))\n",
    "                attributes.append(row['id'])\n",
    "\n",
    "    points_gdf = gpd.GeoDataFrame(data={'id': attributes}, geometry=points, crs= \"EPSG:4326\")\n",
    "    # 2. Create a Voronoi diagram from these points\n",
    "    coords = np.array([point.coords[0] for point in points])\n",
    "    vor = Voronoi(coords)\n",
    "    # 2.bis Convert Voronoi regions to polygons\n",
    "    regions = [r for r in vor.regions if -1 not in r and r != []]\n",
    "    voronoi_polys = [shapely.Polygon([vor.vertices[i] for i in region]) for region in regions]\n",
    "\n",
    "    # 2.ter Create a GeoDataFrame from Voronoi polygons\n",
    "    voronoi_gdf = gpd.GeoDataFrame(geometry=voronoi_polys, crs = \"EPSG:4326\")\n",
    "\n",
    "    # 3. Subtract the buffer polygons from the voronoi\n",
    "    result = gpd.overlay(voronoi_gdf, gdf, how='difference')\n",
    "    # 5. Recombine/dissolve the remaining Voronoi polygons by the unique attribute field\n",
    "    result_with_attr = gpd.sjoin(result, points_gdf, how='left', op='intersects')\n",
    "    dissolved = result_with_attr.dissolve(by='id')\n",
    "\n",
    "    # 6. Clip the dissolved polygons to the bounding box of the world WGS84\n",
    "    bbox = (-180, -90, 180, 90)\n",
    "    land = gpd.GeoDataFrame(data=dissolved, geometry = dissolved.clip_by_rect(*bbox), crs=\"EPSG:4326\")\n",
    "    return land"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "land = get_land_poly(world_oceans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "land_plot = land.hvplot(color = 'index').opts(cmap = 'tab20c', width = 1500, height = 900)\n",
    "land_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = land.merge(world_oceans, on='id')\n",
    "df['geometry'] = df.apply(lambda row: unary_union([row['geometry_x'], row['geometry_y']]), axis=1)\n",
    "whole = gpd.GeoDataFrame(data = df, geometry = df['geometry'], crs = \"EPSG:4326\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "drop unnecessary columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "whole = whole.drop(['geometry_x', 'geometry_y'], axis=1)\n",
    "whole.to_file('assets/world_oceans_land.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "whole.hvplot(color='name').opts(cmap = 'tab20c', width = 1500, height = 900) * land_plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "there are still some imperfections (i.e. overlaps between the polygons). A simplified version has been done using QGIS and is avaiable on this [gist](https://gist.github.com/tomsail/2fa52d9667312b586e7d3baee123b57b):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final = gpd.read_file('https://gist.githubusercontent.com/tomsail/2fa52d9667312b586e7d3baee123b57b/raw/dcda4d7adfc422481cdaf2b74a9dee53e0a505c0/world_maritime_sectors.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "countries = gpd.read_file(gpd.datasets.get_path('naturalearth_lowres'))\n",
    "map_ = countries.hvplot().opts(alpha=0.1, color='white',line_alpha=0.9)\n",
    "final.hvplot(color='ocean').opts(cmap = 'tab20c', width = 1500, height = 900) * map_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "to check if points are inside polygon, we need to install numba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install numba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numba import jit, njit\n",
    "import numba\n",
    "import numpy as np \n",
    "\n",
    "@jit(nopython=True)\n",
    "def pointinpolygon(x,y,poly):\n",
    "    n = len(poly)\n",
    "    inside = False\n",
    "    p2x = 0.0\n",
    "    p2y = 0.0\n",
    "    xints = 0.0\n",
    "    p1x,p1y = poly[0]\n",
    "    for i in numba.prange(n+1):\n",
    "        p2x,p2y = poly[i % n]\n",
    "        if y > min(p1y,p2y):\n",
    "            if y <= max(p1y,p2y):\n",
    "                if x <= max(p1x,p2x):\n",
    "                    if p1y != p2y:\n",
    "                        xints = (y-p1y)*(p2x-p1x)/(p2y-p1y)+p1x\n",
    "                    if p1x == p2x or x <= xints:\n",
    "                        inside = not inside\n",
    "        p1x,p1y = p2x,p2y\n",
    "\n",
    "    return inside\n",
    "\n",
    "\n",
    "@njit(parallel=True)\n",
    "def parallelpointinpolygon(points, polygon):\n",
    "    D = np.empty(len(points), dtype=numba.boolean) \n",
    "    for i in numba.prange(0, len(D)):\n",
    "        D[i] = pointinpolygon(points[i,0], points[i,1], polygon)\n",
    "    return D    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the function 3 functions below consist in:\n",
    "\n",
    " * `reorder_nodes_within_region`: reorder the nodes within a given region based on the computed weights.\n",
    " * `remap_connectivity`: remap the connectivity of the triangles to reflect the new node ordering.\n",
    " * `reorder_mesh`: main functions that translates input mesh to \"ordered\" mesh."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple, List, Iterable\n",
    "import numpy_indexed as npi\n",
    "import geopandas as gpd\n",
    "from inpoly import inpoly2 ## to compare with inpoly2\n",
    "\n",
    "def remap_connectivity(\n",
    "        tri: np.ndarray, \n",
    "        mapping: np.ndarray\n",
    "    ) -> np.ndarray:\n",
    "    \"\"\"Remap the connectivity of a triangular mesh based on the new node order.\n",
    "\n",
    "    Args:\n",
    "        tri: The original connectivity array of the triangular mesh.\n",
    "        mapping: The array that maps old node indices to new ones.\n",
    "\n",
    "    Returns:\n",
    "        The remapped connectivity array for the triangular mesh.\n",
    "    \"\"\"    \n",
    "    remapped_nodes = np.arange(len(mapping))\n",
    "    remapped_triface_nodes = np.c_[\n",
    "        npi.remap(tri[:, 0], mapping, remapped_nodes),\n",
    "        npi.remap(tri[:, 1], mapping, remapped_nodes),\n",
    "        npi.remap(tri[:, 2], mapping, remapped_nodes),\n",
    "    ]\n",
    "    return remapped_triface_nodes\n",
    "\n",
    "## from oceanmesh\n",
    "def get_poly_edges(poly):\n",
    "    col1 = np.arange(0, len(poly) - 1)\n",
    "    col2 = np.arange(1, len(poly))\n",
    "    return np.vstack((col1, col2)).T\n",
    "\n",
    "\n",
    "def reorder_nodes(\n",
    "        x: np.ndarray, \n",
    "        y: np.ndarray, \n",
    "        region_polygon: gpd.GeoDataFrame, \n",
    "        order_wgts: np.ndarray, \n",
    "        method:str = \"inpoly\"\n",
    "    ) -> np.ndarray:\n",
    "    \"\"\"Reorder nodes within a given region based on their weights.\n",
    "\n",
    "    Args:\n",
    "        x: The x-coordinates of the nodes.\n",
    "        y: The y-coordinates of the nodes.\n",
    "        region_polygon: The polygon representing the region.\n",
    "        order_wgts: The weights for ordering the nodes.\n",
    "        method: The method for checking if a point is inside the polygon. (\"inpoly\", \"numba\" or \"bbox\")\n",
    "\n",
    "    Returns:\n",
    "        The indices of the reordered nodes within the given region.\n",
    "    \"\"\"    \n",
    "    if method == \"bbox\": # check inside bbox (faster)\n",
    "        bbox = region_polygon.bounds\n",
    "        points_in_region_final = (y >= bbox[1]) & (y <= bbox[3]) & (x >= bbox[0]) & (x <= bbox[2])\n",
    "    else: # check inside polygon (more accurate)\n",
    "        if region_polygon.geom_type == \"Polygon\":\n",
    "            polygon = np.array(region_polygon.exterior.coords)\n",
    "            if method == \"inpoly\":\n",
    "                e = get_poly_edges(polygon)\n",
    "                points_in_region_final, _ = inpoly2(np.vstack((x,y)).T, polygon, e)    \n",
    "            elif method == \"numba\":\n",
    "                points_in_region_final = parallelpointinpolygon(np.vstack((x,y)).T, polygon)\n",
    "        else : \n",
    "            points_in_region_final = np.zeros(len(x), dtype=np.bool_)\n",
    "            for poly in region_polygon.geoms: \n",
    "                polygon = list(poly.exterior.coords)\n",
    "                if method == \"inpoly\":\n",
    "                    e = get_poly_edges(polygon)\n",
    "                    points_in_region, _ = inpoly2(np.vstack((x,y)).T, polygon, e)    \n",
    "                elif method == \"numba\":\n",
    "                    points_in_region = parallelpointinpolygon(np.vstack((x,y)).T, polygon)\n",
    "                points_in_region_final = np.logical_or(points_in_region_final, points_in_region)\n",
    "    indices_in_region = np.where(points_in_region_final)[0]\n",
    "    order_wgts_in_region = order_wgts[indices_in_region]\n",
    "    idx_sort = np.argsort(order_wgts_in_region)\n",
    "    mapping = np.arange(len(x))\n",
    "    mapping[indices_in_region] = indices_in_region[idx_sort]\n",
    "    return indices_in_region[idx_sort]\n",
    "\n",
    "\n",
    "def reorder_mesh(\n",
    "        x: np.ndarray, \n",
    "        y: np.ndarray, \n",
    "        tri:np.ndarray, \n",
    "        regions: gpd.GeoDataFrame\n",
    "    ) -> Tuple[np.ndarray, np.ndarray, np.ndarray, List[int]]:\n",
    "    \"\"\"Reorder the mesh nodes and remap the connectivity for each region.\n",
    "\n",
    "    Args:\n",
    "        mesh: The dataset representing the mesh.\n",
    "        regions: A GeoDataFrame representing the regions.\n",
    "\n",
    "    Returns:\n",
    "        A tuple containing the reordered x-coordinates, y-coordinates, \n",
    "        remapped connectivity, and the global sorting indices.\n",
    "    \"\"\"    # 1 normalise\n",
    "    normalized_lon = x - np.min(x)\n",
    "    normalized_lat = y - np.min(y)\n",
    "    # 2 compute ordering\n",
    "    order_wgts = (normalized_lon) + (180-normalized_lat) * 360\n",
    "    # 3 test point in regions and fill in mapping / sorted indices\n",
    "    global_sorted = []\n",
    "    for ir, region in regions.iterrows():\n",
    "        region_polygon = region['geometry']\n",
    "        # 4. Reorder the nodes within each region \n",
    "        sorted_indices = reorder_nodes(x,y, region_polygon, order_wgts, method='numba')\n",
    "        global_sorted.extend(sorted_indices)\n",
    "    # 5. Remap the connectivity \n",
    "    tri_out = remap_connectivity(tri, np.array(global_sorted))\n",
    "    return x[global_sorted], y[global_sorted], tri_out, global_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y, tri = ds.lon.values, ds.lat.values, ds.triface_nodes\n",
    "x_, y_, tri_, map_  = reorder_mesh(x, y, tri, final)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "finalise and save the new mesh dataset:\n",
    " * using thalassa's `GENERIC` Format: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mesh_out = xr.Dataset({\n",
    "    'lon': (['node'], x_),\n",
    "    'lat': (['node'], y_),\n",
    "    'triface_nodes': (['triface', 'three'], tri_),\n",
    "    'depth': (['node'], ds.B.isel(time=0).values[map_]),\n",
    "})\n",
    "mesh_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_mesh(x_,y_).opts(width = 1200, height = 600,cmap='tab20c')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "check the depth assignation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "def is_overlapping(tris, meshx):\n",
    "    PIR = 180\n",
    "    x1, x2, x3 = meshx[tris].T\n",
    "    return np.logical_or(abs(x2 - x1) > PIR, abs(x3 - x1) > PIR, abs(x3 - x2) > PIR)\n",
    "\n",
    "m = is_overlapping(tri_ ,x_)\n",
    "fig, ax = plt.subplots(1,1, figsize = (16,9))\n",
    "ax.tricontourf(mesh_out.lon.values, mesh_out.lat.values, mesh_out.triface_nodes[~m], mesh_out.depth, )\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
