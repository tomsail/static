{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b76d2a5e",
   "metadata": {},
   "source": [
    "# Tidal Analysis Comparison: UTide vs PyTides\n",
    "\n",
    "We'll look into this notebook the ways of separating: \n",
    " * the **storm surge** (the meteorologically-driven component) \n",
    " * from the **astronomical tide** (the predictable gravitational component). \n",
    "\n",
    "## Study Location\n",
    "\n",
    "We've chosen **Roscoff, France** as our test case - with some of the largest tidal ranges in Europe (up to 9 meters).\n",
    "from the IOC database, extracted using [`searvey`](https://github.com/seareport/seareport_models), station `rosc`\n",
    "\n",
    "## The data and tools compared: \n",
    "\n",
    "We'll evaluate the following libraries for the (de)tide analysis:\n",
    "\n",
    "1. **[`UTide`](https://github.com/wesleybowman/UTide)** - Python version of the MatLab software\n",
    "2. **[`pytides2`](https://github.com/sahitono/pytides)** - fork from the official repository, working for new versions of python\n",
    "3. **FES2022**. We'll also compare results against the **FES 2022** global tidal model to see how our calculated coefficients stack up against this state-of-the-art reference."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup_section",
   "metadata": {},
   "source": [
    "## Setting Up Our Analysis Toolkit\n",
    "\n",
    "First, let's import the libraries we'll need. Each serves a specific purpose in our tidal detective work:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15defa2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import searvey\n",
    "import shapely\n",
    "import utide\n",
    "from pytides2.tide import Tide\n",
    "import pandas as pd\n",
    "import hvplot.pandas\n",
    "import ioc_cleanup as C"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "config_section",
   "metadata": {},
   "source": [
    "We define the `FES_CONSTITUENTS` - these are the tidal components included in the FES 2022 model, representing the most important tidal harmonics globally.\n",
    "\n",
    "We just reordered the consituent according to their frequencies and importance "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94eda838",
   "metadata": {},
   "outputs": [],
   "source": [
    "UTIDE_OPTS = {\n",
    "    \"constit\": \"auto\", \n",
    "    \"method\": \"ols\", \n",
    "    \"order_constit\": \"frequency\",\n",
    "    \"Rayleigh_min\": 0.97,  # High threshold for constituent resolution\n",
    "    \"verbose\": True\n",
    "}\n",
    "\n",
    "FES_CONSITUENTS = [\n",
    "    \"M2\", \"S2\", \"N2\", \"K2\", \"2N2\", \"L2\", \"T2\", \"R2\", \"NU2\", \"MU2\", \"EPS2\", \"LAMBDA2\", # Semi-diurnal (twice daily)\n",
    "    \"K1\", \"O1\", \"P1\", \"Q1\", \"J1\", \"S1\", # Diurnal (once daily)\n",
    "    \"MF\", \"MM\", \"MSF\", \"SA\", \"SSA\", \"MSQM\", \"MTM\", # Long period (fortnightly to annual)\n",
    "    \"M4\", \"MS4\", \"M6\", \"MN4\", \"N4\", \"S4\", \"M8\", \"M3\", \"MKS2\" # Short period (higher harmonics)\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "functions_section",
   "metadata": {},
   "source": [
    "functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cee3c3f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_availability(series: pd.Series, freq=\"60min\") -> float:\n",
    "    resampled = series.resample(freq).mean()\n",
    "    data_avail_ratio = 1 - resampled.isna().sum() / len(resampled)\n",
    "    return float(data_avail_ratio)\n",
    "\n",
    "\n",
    "def utide_get_coefs(ts: pd.Series, lat: float, resample: int = None)-> dict: \n",
    "    UTIDE_OPTS[\"lat\"] = lat\n",
    "    if resample is not None:\n",
    "        ts = ts.resample(f\"{resample}min\").mean()\n",
    "        ts = ts.shift(freq=f\"{resample / 2}min\")  # Center the resampled points\n",
    "    return utide.solve(ts.index, ts, **UTIDE_OPTS)\n",
    "\n",
    "\n",
    "def utide_surge(ts: pd.Series, lat: float, resample: int = None)-> pd.Series: \n",
    "    ts0 = ts.copy()\n",
    "    coef = utide_get_coefs(ts, lat, resample)\n",
    "    tidal = utide.reconstruct(ts0.index, coef, verbose = UTIDE_OPTS[\"verbose\"])\n",
    "    return pd.Series(data=ts0.values - tidal.h, index = ts0.index)\n",
    "\n",
    "\n",
    "def pytide_get_coefs(ts: pd.Series, resample: int = None) -> dict:\n",
    "    if resample is not None:\n",
    "        ts = ts.resample(f\"{resample}min\").mean()\n",
    "        ts = ts.shift(freq=f\"{resample / 2}min\")  # Center the resampled points\n",
    "    ts = ts.dropna()\n",
    "    return Tide.decompose(ts, ts.index.to_pydatetime())[0]\n",
    "\n",
    "\n",
    "def pytides_surge(ts: pd.Series, resample: int = None)-> pd.Series:\n",
    "    ts0 = ts.copy()\n",
    "    tide = pytide_get_coefs(ts, resample)\n",
    "    t0 = ts.index.to_pydatetime()[0]\n",
    "    hours = (ts.index - ts.index[0]).total_seconds()/3600\n",
    "    times = Tide._times(t0, hours)\n",
    "    return pd.Series(ts0.values - tide.at(times), index=ts0.index)\n",
    "\n",
    "\n",
    "def reduce_coef_to_fes(df: pd.DataFrame):\n",
    "    res = pd.DataFrame(0.0, index=FES_CONSITUENTS, columns=df.columns)\n",
    "    common_constituents = df.index.intersection(FES_CONSITUENTS)\n",
    "    res.loc[common_constituents] = df.loc[common_constituents]\n",
    "    \n",
    "    # Report what's different from FES\n",
    "    not_in_fes_df = df[~df.index.isin(FES_CONSITUENTS)]\n",
    "    not_in_fes = not_in_fes_df.index.tolist()\n",
    "    not_in_fes_amps = not_in_fes_df[\"amplitude\"].round(3).tolist()\n",
    "    missing_fes = set(FES_CONSITUENTS) - set(df.index)\n",
    "    \n",
    "    # print(f\"Constituents found but not in FES: {not_in_fes}\")\n",
    "    # print(f\"Their amplitudes: {not_in_fes_amps}\")\n",
    "    # if missing_fes:\n",
    "    #     print(f\"FES constituents missing from analysis (set to 0): {sorted(missing_fes)}\")\n",
    "    \n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "station_setup",
   "metadata": {},
   "source": [
    "study site"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21592138",
   "metadata": {},
   "outputs": [],
   "source": [
    "station = \"rosc\"\n",
    "sensor = \"rad\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "data_fetch",
   "metadata": {},
   "source": [
    "get 25 years of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66f0764f",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw = searvey.fetch_ioc_station( \n",
    "    station, \n",
    "    \"2000-01-01\", \n",
    "    pd.Timestamp.now()\n",
    ")\n",
    "raw.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "station_info",
   "metadata": {},
   "source": [
    "Station Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddb90f4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get station metadata\n",
    "ioc_df = searvey.get_ioc_stations()\n",
    "_lat = ioc_df[ioc_df.ioc_code == station].lat.values[0]\n",
    "\n",
    "station_info = ioc_df[ioc_df.ioc_code == station]\n",
    "print(f\"Station: {station_info['location'].values[0]}\")\n",
    "print(f\"Latitude: {_lat:.4f}Â°N\")\n",
    "print(f\"Longitude: {station_info['lon'].values[0]:.4f}Â°E\")\n",
    "print(f\"Country: {station_info['country'].values[0]}\")\n",
    "print(\"\\nFull station details:\")\n",
    "station_info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "data_cleaning",
   "metadata": {},
   "source": [
    "let's clean the data, using [ioc_cleanup](https://github.com/seareport/ioc_cleanup/tree/ioc_2024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0df3ba85",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p transformations\n",
    "import requests\n",
    "response = requests.get(f'https://raw.githubusercontent.com/seareport/ioc_cleanup/refs/heads/ioc_2024/transformations/{station}_{sensor}.json')\n",
    "with open(f'transformations/{station}_{sensor}.json', 'wb') as f:\n",
    "    f.write(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "transformation_preview",
   "metadata": {},
   "source": [
    "here's a snapshot at the cleaning trasnformation file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5c76ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "! head -20 \"transformations/{station}_{sensor}.json\" "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "apply_qc",
   "metadata": {},
   "source": [
    "Now let's apply these transformations to clean our data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89b6f2e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and apply quality control transformations\n",
    "trans = C.load_transformation(station, sensor)\n",
    "cleaned_data = C.transform(raw, trans)\n",
    "ts = cleaned_data[sensor]\n",
    "\n",
    "print(f\"Data cleaning complete!\")\n",
    "print(f\"Original data points: {len(raw)}\")\n",
    "print(f\"Cleaned data points: {len(ts)}\")\n",
    "print(f\"Data availability: {data_availability(ts):.1%}\")\n",
    "print(f\"Time range raw: {raw.index.min()} to {raw.index.max()}\")\n",
    "print(f\"Time range clean: {ts.index.min()} to {ts.index.max()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "utide_analysis",
   "metadata": {},
   "source": [
    "## 1: UTide Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19de8026",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = utide_get_coefs(ts, _lat, resample=20)\n",
    "print(f\"Found {len(out['name'])} tidal constituents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "utide_results",
   "metadata": {},
   "source": [
    "Let's organize the UTide results into a clean DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "460f8bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def utide_to_df(utide_coef: utide.utilities.Bunch) -> pd.DataFrame:\n",
    "    return pd.DataFrame({ \n",
    "        \"amplitude\": utide_coef[\"A\"],\n",
    "        \"phase\": utide_coef[\"g\"],\n",
    "        \"amplitude_CI\": utide_coef[\"A_ci\"],\n",
    "        \"phase_CI\": utide_coef[\"g_ci\"]\n",
    "    }, index=utide_coef[\"name\"])\n",
    "\n",
    "print(\"Top 20 tidal constituents by amplitude (UTide):\")\n",
    "print(utide_to_df(out).sort_values('amplitude', ascending=False).head(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pytides_analysis",
   "metadata": {},
   "source": [
    "## Round 2: PyTides Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c0dc4ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_pytides = pytide_get_coefs(ts, 20)\n",
    "print(f\"Found {len(out_pytides.model['constituent'])} tidal constituents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pytides_results",
   "metadata": {},
   "source": [
    "Let's organize the PyTides results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "563d4963",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pytides_to_df(pytides_tide: Tide)-> pd.DataFrame:\n",
    "    constituent_names = [c.name.upper() for c in pytides_tide.model['constituent']]\n",
    "    return pd.DataFrame(pytides_tide.model, index=constituent_names).drop('constituent', axis=1)\n",
    "\n",
    "print(\"Top 20 tidal constituents by amplitude (PyTides):\")\n",
    "print(pytides_to_df(out_pytides).sort_values('amplitude', ascending=False).head(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "comparison_prep",
   "metadata": {},
   "source": [
    "## Comparison\n",
    "To fairly compare UTide and pytides results, we'll standardize them against the FES 2022 constituent list. This will show us:\n",
    "\n",
    "1. Which constituents each method found\n",
    "2. Which constituents are missing from each analysis\n",
    "3. How the amplitudes compare for common constituents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1efe4ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "pytides_reduced_coef = reduce_coef_to_fes(pytides_to_df(out_pytides))\n",
    "pytides_reduced_coef.head(10)\n",
    "\n",
    "utide_reduced_coef = reduce_coef_to_fes(utide_to_df(out))\n",
    "utide_reduced_coef.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "visual_comparison",
   "metadata": {},
   "source": [
    "### visual comparison\n",
    "\n",
    "**What to look for:**\n",
    "- **Major constituents** (M2, S2, N2, K1, O1) should have similar amplitudes\n",
    "- **Minor constituents** may show more variation between methods\n",
    "- **Missing constituents** appear as zero amplitude in one method but not the other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef00c71b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat_utide_pytides(pytides_df, utide_df):\n",
    "    multi_df = pd.concat({\"pytides\": pytides_df, \"utide\": utide_df})\n",
    "    multi_df.index.names = ['method', 'constituent']\n",
    "    multi_df = multi_df.swaplevel().sort_index()\n",
    "\n",
    "    available_constituents = multi_df.index.get_level_values('constituent').unique()\n",
    "    filtered_order = [c for c in FES_CONSITUENTS if c in available_constituents][::-1]\n",
    "    return  multi_df.reindex(filtered_order, level='constituent')\n",
    "\n",
    "multi_df_ordered = concat_utide_pytides(pytides_reduced_coef, utide_reduced_coef)\n",
    "multi_df_ordered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca3b8fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the comparison plot\n",
    "\n",
    "def plot_comparative_amplitudes(df):\n",
    "    return df.amplitude.hvplot.barh(\n",
    "        ylabel=\"Tidal Constituent\",\n",
    "        xlabel=\"Amplitude (meters)\", \n",
    "        by=\"method\", \n",
    "        grid=True,\n",
    "        title=f\"Tidal Amplitudes: UTide vs PyTide, station {station}\",\n",
    "        legend='top_right',\n",
    "        rot=90\n",
    "    ).opts(\n",
    "        height=1000, \n",
    "        width=1000,\n",
    "        fontsize={'title': 15, 'labels': 12, 'xticks': 8, 'yticks': 8}\n",
    "    )\n",
    "\n",
    "plot_comparative_amplitudes(multi_df_ordered)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "944890cb",
   "metadata": {},
   "source": [
    "### Quantitave comparison\n",
    "\n",
    "we'll assess the RSS between the all the consituents, taking pytide as the reference: \n",
    "\n",
    "RSS is given by: [1]\n",
    "\n",
    "$$\n",
    "\\operatorname{RSS} = \\sum_{i=1}^{n} \\left(A_{pytides,i} - A_{utide,i}\\right)^2\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "180dac65",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_rss(df): \n",
    "    amp_pytides = df.xs('pytides', level='method')['amplitude']\n",
    "    amp_utide = df.xs('utide', level='method')['amplitude']\n",
    "    # Ensure both Series are aligned by index\n",
    "    amp_pytides, amp_utide = amp_pytides.align(amp_utide, join='inner')\n",
    "    # Compute RSS\n",
    "    return ((amp_pytides - amp_utide) ** 2).sum()\n",
    "\n",
    "print(f\"rss for {station} is {compute_rss(multi_df_ordered):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef9d0036",
   "metadata": {},
   "source": [
    "we'll iterate though an existing folder, contaning clean data at tide gauge locations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ac40cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_FOLDER = \"data\"\n",
    "UTIDE_OPTS[\"verbose\"] = False\n",
    "import glob\n",
    "import os \n",
    "\n",
    "\n",
    "res = {}\n",
    "for path in glob.glob(\"data/*parquet\"): \n",
    "    ts = pd.read_parquet(path)\n",
    "    ts = ts[ts.columns[0]]\n",
    "    root, file_ext = os.path.split(path)\n",
    "    file, ext = os.path.splitext(file_ext)\n",
    "    station, sensor = file.split(\"_\")\n",
    "    _lon = ioc_df[ioc_df.ioc_code == station].lon.values[0]\n",
    "    _lat = ioc_df[ioc_df.ioc_code == station].lat.values[0]\n",
    "    try: \n",
    "        ut = utide_get_coefs(ts, _lat, resample=20)\n",
    "        utide_reduced_coef = reduce_coef_to_fes(utide_to_df(ut))\n",
    "        pt = pytide_get_coefs(ts, 20.)\n",
    "        pytides_reduced_coef = reduce_coef_to_fes(pytides_to_df(pt))\n",
    "        multi_df_ordered = concat_utide_pytides(pytides_reduced_coef, utide_reduced_coef)\n",
    "        rss = compute_rss(multi_df_ordered)\n",
    "        res[station] = {\n",
    "            \"ioc_code\": station,\n",
    "            \"lat\": _lat,\n",
    "            \"lon\": _lon,\n",
    "            \"rss\": rss\n",
    "        }\n",
    "        print(f\"rss for {station} is {rss:.4f}\")\n",
    "    except Exception as e:\n",
    "        print(f\"couldn't process {station}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5bbb504",
   "metadata": {},
   "outputs": [],
   "source": [
    "rss_df = pd.DataFrame(res).T\n",
    "\n",
    "rss_df.rss = rss_df.rss.astype(float)\n",
    "rss_df.hvplot.points(\n",
    "    x=\"lon\",\n",
    "    y=\"lat\",\n",
    "    c=\"rss\",\n",
    "    hover_cols = ['ioc_code'],\n",
    "    s=100,\n",
    "    geo = True,\n",
    "    tiles = True\n",
    ").opts(width = 1000, height = 800, title = \"RSS difference between UTide and pytides constituents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "606f4ad2",
   "metadata": {},
   "source": [
    "plot tidal amplitude from station:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f75c17eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "station= \"delf\"\n",
    "sensor = \"flt\"\n",
    "ioc_df = searvey.get_ioc_stations()\n",
    "\n",
    "rsp = 20\n",
    "\n",
    "_lat = ioc_df[ioc_df.ioc_code == station].lat.values[0]\n",
    "ts = pd.read_parquet(f\"data/{station}_{sensor}.parquet\")[sensor]\n",
    "out_pytides = pytide_get_coefs(ts, rsp)\n",
    "pytides_reduced_coef = reduce_coef_to_fes(pytides_to_df(out_pytides))\n",
    "out_utides = utide_get_coefs(ts, _lat, resample=rsp)\n",
    "utide_reduced_coef = reduce_coef_to_fes(utide_to_df(out_utides))\n",
    "multi_df_ordered = concat_utide_pytides(pytides_reduced_coef, utide_reduced_coef)\n",
    "compute_rss(multi_df_ordered)\n",
    "plot_comparative_amplitudes(multi_df_ordered)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "surge_analysis",
   "metadata": {},
   "source": [
    "### comparison between tidal residuals\n",
    "\n",
    "If both methods are working correctly, the tidal residuals - corresponding to the meteorological component - time series should be very similar. \n",
    "\n",
    "Significant differences would indicate problems with `utide`, `pytides` or both approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dc3a1ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Calculating storm surge using both methods...\")\n",
    "rsp = 30\n",
    "surge_pytides = pytides_surge(ts, resample=rsp)\n",
    "surge_utide = utide_surge(ts, _lat, resample=rsp)\n",
    "\n",
    "correlation = surge_pytides.corr(surge_utide)\n",
    "rmse = ((surge_pytides - surge_utide)**2).mean()**0.5\n",
    "\n",
    "print(f\"--------\\nðŸ“Š Storm Surge Comparison Results:\")\n",
    "print(f\"Correlation coefficient: {correlation:.4f}\")\n",
    "print(f\"RMSE between methods: {rmse:.3f} meters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6ce5a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "(surge_pytides.resample(\"1h\").mean().hvplot(label=\"sugre pytides\", grid=True)\n",
    " *surge_utide.resample(\"1h\").mean().hvplot(label=\"surge utide\")\n",
    " ).opts(\n",
    "    width=1200,\n",
    "    height = 500\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea9b4a21",
   "metadata": {},
   "source": [
    "## Second part: chunked detiding (to be continued)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a607f1a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def surge_chunked(ts: pd.Series, lat: float, resample: int = None, max_days: int = 365) -> pd.Series:\n",
    "    ts0 = ts.copy()\n",
    "    if resample is not None:\n",
    "        ts = ts.resample(f\"{resample}min\").mean()\n",
    "        ts = ts.shift(freq=f\"{resample / 2}min\")\n",
    "\n",
    "    OPTS = {\n",
    "        \"constit\": \"auto\",\n",
    "        \"method\": \"ols\",\n",
    "        \"order_constit\": \"frequency\",\n",
    "        \"Rayleigh_min\": 0.97,\n",
    "        \"lat\": lat,\n",
    "        \"verbose\": True\n",
    "    }\n",
    "\n",
    "    detided = pd.Series(index=ts0.index, dtype='float64')\n",
    "\n",
    "    t_start = ts.index.min()\n",
    "    t_end = ts.index.max()\n",
    "    chunk_start = t_start\n",
    "    chunk_size = pd.Timedelta(days = max_days)\n",
    "\n",
    "    while chunk_start < t_end:\n",
    "        current_chunk_size = chunk_size\n",
    "\n",
    "        while True:\n",
    "            chunk_end = chunk_start + current_chunk_size\n",
    "            if chunk_end > t_end:\n",
    "                chunk_end = t_end\n",
    "\n",
    "            chunk = ts[chunk_start:chunk_end]\n",
    "            avail = data_availability(chunk, freq=\"60min\")\n",
    "            total_days = current_chunk_size.total_seconds()/(3600*24)\n",
    "            if total_days*avail >= 365*0.9:\n",
    "                print(f\"Detiding chunk {chunk_start.date()} to {chunk_end.date()} ({avail*100:.1f}% available)\")\n",
    "                try:\n",
    "                    coef = utide.solve(\n",
    "                        chunk.index,\n",
    "                        chunk,\n",
    "                        **OPTS\n",
    "                    )\n",
    "                    recon_index = ts0.loc[chunk_start:chunk_end].index\n",
    "                    tidal = utide.reconstruct(recon_index, coef, verbose=OPTS[\"verbose\"])\n",
    "                    detided.loc[chunk_start:chunk_end] = ts0.loc[chunk_start:chunk_end].values - tidal.h\n",
    "                except Exception as e:\n",
    "                    print(f\"UTide failed on chunk {chunk_start} to {chunk_end}: {e}\")\n",
    "                break\n",
    "            else:\n",
    "                print(f\"Data availability {avail:.1f}% from {chunk_start.date()} to {chunk_end.date()} â€” expanding chunk.\")\n",
    "                current_chunk_size += pd.Timedelta(days=6*30)\n",
    "                if chunk_start + current_chunk_size > t_end:\n",
    "                    print(\"End of time series reached with insufficient data.\")\n",
    "                    break\n",
    "\n",
    "        chunk_start = chunk_end\n",
    "\n",
    "    return detided"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3246b51a",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunked = surge_chunked(ts, _lat, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "466fd0e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "(surge_pytides.resample(\"1h\").mean().hvplot(\n",
    "    label=\"sugre pytides\", grid=True\n",
    " )*surge_utide.resample(\"1h\").mean().hvplot(\n",
    "    label=\"surge utide\"\n",
    " )*chunked.resample(\"1h\").mean().hvplot(\n",
    "    label=\"chunked utide\"\n",
    " )).opts(\n",
    "    width=1200,\n",
    "    height = 500\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
