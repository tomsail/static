{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Meteo conversion optimisation for hydro models\n",
    "\n",
    "We will go in details of how to convert meteorological data (ECMWF/ERA5): \n",
    " * from netcdf gridded data \n",
    "\n",
    " ![ECMWF](https://climate.copernicus.eu/sites/default/files/inline-images/CERRA_wind.png)\n",
    " * onto zarr unstrutured meshes format \n",
    " \n",
    " ![global mesh](assets/ERA5_wind.gif)\n",
    "\n",
    "The idea is to able to prepare the data to be able to use it as an input for our hydro models (SCHISM & TELEMAC) used at JRC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Sensitivity on the chunking size\n",
    "\n",
    "### Important! first write an empty array\n",
    "> Dimensions cannot be included in both `region` and `append_dim` at the same time. To create empty arrays to fill in with `region`, use a separate call to `to_zarr()` with `compute=False.`\n",
    "\n",
    "the first step consists in creating an initial Zarr dummy Dataset array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.array as da\n",
    "import numpy as np\n",
    "import hvplot.pandas\n",
    "import pandas as pd\n",
    "import holoviews as hv\n",
    "from holoviews import opts\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import shutil\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up parameters for the dataset\n",
    "NTIMES = 744     # one month hourly\n",
    "NNODES = 100000  # 100k nodes\n",
    "chunk_size_time = 10  # Choose an appropriate chunk size for the time dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zero = da.empty((NTIMES, NNODES), chunks=(chunk_size_time, NNODES))\n",
    "zero"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "what about if we had a dimension for the variables?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NVAR = 4\n",
    "cube_zero = da.empty((NTIMES,NVAR, NNODES), chunks=(chunk_size_time, 1, NNODES))\n",
    "cube_zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the data to a temporary Zarr store\n",
    "tmp = 'tmp.zarr'\n",
    "ds = xr.Dataset({\n",
    "    \"u10\": (('time', 'node'), zero),\n",
    "    \"v10\": (('time', 'node'), zero),\n",
    "    \"msl\": (('time', 'node'), zero),\n",
    "    \"tmp\": (('time', 'node'), zero),\n",
    "     \n",
    "})\n",
    "ds.to_zarr(tmp, compute=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we generated a metadata file, which will help us to aggregate the data later on. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat tmp.zarr/.zmetadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we'll put everything in a function to calculate the time taken to write data, depending on : \n",
    " * the chunk size on the `time` dimention \n",
    " * the chunk size on the `node` dimention "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_chunk_size(time_chunk, node_chunk):\n",
    "    start = time.time()\n",
    "    tmp = 'tmp.zarr'\n",
    "    if os.path.exists(tmp):\n",
    "        shutil.rmtree(tmp)\n",
    "    # create attrs and file \n",
    "    zero = da.empty((NTIMES, NNODES), chunks=(time_chunk, node_chunk))\n",
    "    bytes_per_element = zero.dtype.itemsize\n",
    "    chunk_size_mb = time_chunk * node_chunk * bytes_per_element  / 1024 / 1000\n",
    "    if chunk_size_mb > 2000: \n",
    "        return np.nan, np.nan\n",
    "    else: \n",
    "        ds = xr.Dataset({\n",
    "        \"u10\": (('time', 'node'), zero),\n",
    "        \"v10\": (('time', 'node'), zero),\n",
    "        \"msl\": (('time', 'node'), zero),\n",
    "        \"tmp\": (('time', 'node'), zero), \n",
    "        }).to_zarr(tmp, compute=False)\n",
    "        # Create a Pandas date range for the time coordinates\n",
    "        time_range = pd.date_range(start=pd.Timestamp(2023, 1, 1), periods=NTIMES, freq='h')\n",
    "        # Loop over variables\n",
    "        count = 0\n",
    "        for var_name in [\"u10\", \"v10\", \"msl\", \"tmp\"]:\n",
    "            # Loop over time in chunks\n",
    "            for t_start in range(0, NTIMES, time_chunk):\n",
    "                t_end = min(t_start + time_chunk, NTIMES)\n",
    "                time_chunk = time_range[t_start:t_end]\n",
    "                # Loop over nodes in chunks\n",
    "                for i_node in range(0, NNODES, node_chunk):\n",
    "                    end_node = min(i_node + node_chunk, NNODES)\n",
    "                    data_chunk = np.ones((t_end - t_start) * (end_node - i_node))\n",
    "                    data_chunk = data_chunk.reshape((t_end - t_start, end_node - i_node))\n",
    "                    node_chunk = np.arange(i_node, end_node)\n",
    "                    coords = {'node': node_chunk, 'time':time_chunk}\n",
    "                    ds = xr.Dataset({var_name: (('time', 'node'), data_chunk)}, coords=coords)\n",
    "                    region = {'time': slice(t_start, t_end), 'node': slice(i_node, end_node)}\n",
    "                    ds.to_zarr(tmp, mode=\"a-\", region=region)\n",
    "                    count += 1\n",
    "        \n",
    "        end = time.time()\n",
    "        print(f\"Chunk size: times:{time_chunk} x nodes:{node_chunk} - {chunk_size_mb}kB\")\n",
    "        return end - start, chunk_size_mb\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iterate for a range of chunk sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunksizes =  [10, 50, 100, 150, 200, 250, 300, 350, 400, 500]\n",
    "nodessizes =  [5000, 10000, 50000, 100000] #, 500000, 1000000]\n",
    "\n",
    "time_perf = np.zeros((len(chunksizes), len(nodessizes)))\n",
    "sizes = np.zeros((len(chunksizes), len(nodessizes)))\n",
    "for i_t, chunktime in enumerate(chunksizes): \n",
    "    for i_n, chunknode in enumerate(nodessizes):\n",
    "        perf, size = test_chunk_size(chunktime, chunknode)\n",
    "        time_perf[i_t,i_n] = perf\n",
    "        sizes[i_t,i_n] = size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert data for pcolormesh plots into a format compatible with hvplot\n",
    "time_perf_mesh_df = pd.DataFrame(time_perf, index=chunksizes, columns=nodessizes)\n",
    "sizes_mesh_df = pd.DataFrame(sizes, index=chunksizes, columns=nodessizes)\n",
    "\n",
    "# Convert data for line plot into a Pandas DataFrame\n",
    "sizes_flat = sizes.ravel()\n",
    "time_perf_flat = time_perf.ravel()\n",
    "idx = np.argsort(sizes_flat)\n",
    "line_df = pd.DataFrame({\n",
    "    'Size (MB)': sizes_flat[idx],\n",
    "    'Time (sec)': time_perf_flat[idx]\n",
    "})\n",
    "\n",
    "# Create hvplot pcolormesh plots\n",
    "time_perf_plot = time_perf_mesh_df.hvplot.heatmap(\n",
    "    x='columns', y='index', C='value', logx=True, colorbar=True, xlabel=\"chunk size for nodes\", ylabel = \"chunk size for times\", \n",
    "    title='Time taken for each chunk (in seconds)', width = 500)\n",
    "\n",
    "sizes_plot = sizes_mesh_df.hvplot.heatmap(\n",
    "    x='columns', y='index', C='value', logx=True, colorbar=True, xlabel=\"chunk size for nodes\", ylabel = \"chunk size for times\", \n",
    "    title='Size of each chunk (in MB)', width = 500)\n",
    "\n",
    "# Create hvplot line plot\n",
    "line_plot = line_df.hvplot.line(\n",
    "    x='Size (MB)', y='Time (sec)', \n",
    "    title='Time taken vs. Size of each chunk', width = 500)\n",
    "\n",
    "# Combine plots into a layout\n",
    "layout = (time_perf_plot + sizes_plot + line_plot).cols(3)\n",
    "\n",
    "# Apply some options for better layout\n",
    "layout.opts(\n",
    "    opts.HeatMap(colorbar=True),\n",
    "    opts.Layout(shared_axes=False, merge_tools=False),\n",
    ")\n",
    "# # Display the layout\n",
    "hv.output(layout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusion: that the bigger the chunk size is, the smaller is the time to process the data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - test with real data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We used previously the `region` parameter to append chunks to the zarr file.\n",
    "\n",
    "According to the [docs](https://docs.xarray.dev/en/stable/user-guide/io.html#modifying-existing-zarr-stores), \n",
    "> region (`dict` or `\"auto\"`, optional) – Optional mapping from dimension names to integer slices along dataset dimensions to indicate the region of existing zarr array(s) in which to write this dataset’s data. For example, `{'x': slice(0, 1000), 'y': slice(10000, 11000)}` would indicate that values should be written to the region `0:1000` along `x` and `10000:11000` along `y`.\n",
    "\n",
    "**Conclusion**: We can only enter **monotonic** slices in the `region` dictionary argument. That means we need specify the output chunks with monotomic slices of a **constant size**.\n",
    "\n",
    "To have coherent geographic we need to reorder the nodes along their coordinates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 load mesh file\n",
    "we will use the `xarray-selafin` package to load the mesh file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! poetry add xarray-selafin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load mesh file\n",
    "mesh = xr.open_dataset(\"global-v0.slf\", engine = 'selafin')\n",
    "mesh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 split and reorder along regions "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let's visualise the mesh nodes number along the lat/lon dimensions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_mesh(x, y): \n",
    "    df = pd.DataFrame({'x': x, 'y': y, 'id': np.arange(len(x))})\n",
    "    im = df.hvplot.points(x='x', y='y', c='id',s=10)\n",
    "    return im\n",
    "\n",
    "x, y = mesh.x.values, mesh.y.values\n",
    "plot_mesh(x,y).opts(width = 1200, height = 600,cmap='tab20c')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "out of the mesher, mesh nodes are not sorted, we need to reorder them.\n",
    "\n",
    "The methodology will proceed as following : \n",
    "\n",
    "1. Normalize the longitude (x) and latitude (y) coordinates of the nodes.\n",
    "2. Compute the ordering weights for the nodes.\n",
    "3. For each region, determine which nodes fall within that region.\n",
    "4. Reorder the nodes within each region based on the computed weights.\n",
    "5. Remap the connectivity of the triangles to reflect the new node ordering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "first we'll load the regions from a `json` file we previously created on `QGIS`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"world_regions.json\") as f:\n",
    "    json_regions = json.load(f)\n",
    "json_regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import hvplot.pandas  # This import is necessary to use the hvplot accessor\n",
    "countries = gpd.read_file(gpd.datasets.get_path(\"naturalearth_lowres\"))\n",
    "world_regions = gpd.read_file(\"world_regions.json\")\n",
    "countries_plot = countries.hvplot(color='k')\n",
    "world_regions_plot = world_regions.hvplot(\n",
    "    alpha=0.6, \n",
    "    c='id', \n",
    "    cmap='tab20c',\n",
    "    hover_cols=['id','name'],)\n",
    "overlay = countries_plot * world_regions_plot\n",
    "overlay.opts(\n",
    "    width = 1200,\n",
    "    height = 600,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3 functions below: \n",
    "1. `reorder_nodes_within_region`: reorder the nodes within a given region based on the computed weights.\n",
    "2. `remap_connectivity`: remap the connectivity of the triangles to reflect the new node ordering.\n",
    "3. `reorder_mesh`: main functions that translates input mesh to \"ordered\" mesh. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple, List, Iterable\n",
    "import numpy_indexed as npi\n",
    "\n",
    "def remap_connectivity(\n",
    "        tri: np.ndarray, \n",
    "        mapping: np.ndarray\n",
    "    ) -> np.ndarray:\n",
    "    \"\"\"Remap the connectivity of a triangular mesh based on the new node order.\n",
    "\n",
    "    Args:\n",
    "        tri: The original connectivity array of the triangular mesh.\n",
    "        mapping: The array that maps old node indices to new ones.\n",
    "\n",
    "    Returns:\n",
    "        The remapped connectivity array for the triangular mesh.\n",
    "    \"\"\"    \n",
    "    remapped_nodes = np.arange(len(mapping))\n",
    "    remapped_triface_nodes = np.c_[\n",
    "        npi.remap(tri[:, 0], mapping, remapped_nodes),\n",
    "        npi.remap(tri[:, 1], mapping, remapped_nodes),\n",
    "        npi.remap(tri[:, 2], mapping, remapped_nodes),\n",
    "    ]\n",
    "    return remapped_triface_nodes\n",
    "\n",
    "\n",
    "def reorder_nodes_within_region(\n",
    "        x: np.ndarray, \n",
    "        y: np.ndarray, \n",
    "        region_polygon: gpd.GeoDataFrame, \n",
    "        order_wgts: np.ndarray\n",
    "    ) -> np.ndarray:\n",
    "    \"\"\"Reorder nodes within a given region based on their weights.\n",
    "\n",
    "    Args:\n",
    "        x: The x-coordinates of the nodes.\n",
    "        y: The y-coordinates of the nodes.\n",
    "        region_polygon: The polygon representing the region.\n",
    "        order_wgts: The weights for ordering the nodes.\n",
    "\n",
    "    Returns:\n",
    "        The indices of the reordered nodes within the given region.\n",
    "    \"\"\"    \n",
    "    bbox = region_polygon.bounds\n",
    "    points_in_region = (y >= bbox[1]) & (y <= bbox[3]) & (x >= bbox[0]) & (x <= bbox[2])\n",
    "    indices_in_region = np.where(points_in_region)[0]\n",
    "    order_wgts_in_region = order_wgts[indices_in_region]\n",
    "    idx_sort = np.argsort(order_wgts_in_region)\n",
    "    mapping = np.arange(len(x))\n",
    "    mapping[indices_in_region] = indices_in_region[idx_sort]\n",
    "    return indices_in_region[idx_sort]\n",
    "\n",
    "\n",
    "def reorder_mesh(\n",
    "        x: np.ndarray, \n",
    "        y: np.ndarray, \n",
    "        tri:np.ndarray, \n",
    "        regions: gpd.GeoDataFrame\n",
    "    ) -> Tuple[np.ndarray, np.ndarray, np.ndarray, List[int]]:\n",
    "    \"\"\"Reorder the mesh nodes and remap the connectivity for each region.\n",
    "\n",
    "    Args:\n",
    "        mesh: The dataset representing the mesh.\n",
    "        regions: A GeoDataFrame representing the regions.\n",
    "\n",
    "    Returns:\n",
    "        A tuple containing the reordered x-coordinates, y-coordinates, \n",
    "        remapped connectivity, and the global sorting indices.\n",
    "    \"\"\"    # 1 normalise\n",
    "    normalized_lon = mesh.x - np.min(mesh.x)\n",
    "    normalized_lat = mesh.y - np.min(mesh.y)\n",
    "    # 2 compute ordering\n",
    "    order_wgts = (normalized_lon) + (180-normalized_lat) * 360\n",
    "    # 3 test point in regions and fill in mapping / sorted indices\n",
    "    global_sorted = []\n",
    "    for ir, region in regions.iterrows():\n",
    "        region_polygon = region['geometry']\n",
    "        # 4. Reorder the nodes within each region \n",
    "        sorted_indices = reorder_nodes_within_region(x,y, region_polygon, order_wgts)\n",
    "        global_sorted.extend(sorted_indices)\n",
    "    # 5. Remap the connectivity \n",
    "    tri_out = remap_connectivity(tri, np.array(global_sorted))\n",
    "    return x[global_sorted], y[global_sorted], tri_out, global_sorted\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y, tri = mesh.x.values, mesh.y.values, mesh.attrs['ikle2'] - 1\n",
    "x_, y_, tri_, map_  = reorder_mesh(x, y, tri, world_regions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test if the renumbering worked as expected: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_mesh(x_, y_).opts(width = 1200, height = 600, cmap = 'tab20c')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test if the triangle remapping worked as expected: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "fig, ax = plt.subplots(1,1, figsize = (12,6))\n",
    "def is_overlapping(tris, meshx):\n",
    "    PIR = 180\n",
    "    x1, x2, x3 = meshx[tris].T\n",
    "    return np.logical_or(abs(x2 - x1) > PIR, abs(x3 - x1) > PIR, abs(x3 - x2) > PIR)\n",
    "m = is_overlapping(tri_ ,x_)\n",
    "ax.tricontourf(x_, y_, tri_[~m], np.arange(len(x)),cmap = 'tab20c')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### finalise and save the new mesh dataset: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "using [thalassa](https://github.com/ec-jrc/Thalassa/blob/b9d977cd6999e73f5ad884e9e7b96d4041b60827/thalassa/normalization.py#L27)'s `GENERIC` Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mesh_out = xr.Dataset({\n",
    "    'lon': (['node'], x_),\n",
    "    'lat': (['node'], y_),\n",
    "    'triface_nodes': (['triface', 'three'], tri_),\n",
    "    'depth': (['node'], mesh.B.isel(time=0).values[map_]),\n",
    "})\n",
    "mesh_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "check the depth assignation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "fig, ax = plt.subplots(1,1, figsize = (12,6))\n",
    "ax.tricontourf(x_, y_, tri_[~m], mesh_out.depth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 interpolate meteo and append on to zarr file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we will load ERA5 reanalysis data and test the chunking size on the `time` and `node` dimensions: \n",
    " * for the `time` dimension, the chunking is quite  straightforward.\n",
    " * for the `node` dimension, the chunking gets more complex because we deal with 2 different meshes. we will explain why in details below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load  ERA5 reanalysis data\n",
    "era5 = xr.open_dataset(\"era5_2023_uvp.nc\")\n",
    "era5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "first we will the interpolation functions used in TELEMAC ( [`optim_gen-atm`](https://gitlab.pam-retd.fr/otm/telemac-mascaret/-/blob/optim-gen-atm/scripts/python3/utils/geometry.py) branch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial import Delaunay, cKDTree\n",
    "def get_weights(in_xy, out_xy, d=2):\n",
    "    t = Delaunay(in_xy) # triangulate output mesh\n",
    "    s = t.find_simplex(out_xy) \n",
    "    vert = np.take(t.simplices, np.maximum(s, 0), axis=0)  # Use max to avoid negative indices\n",
    "    t_ = np.take(t.transform, np.maximum(s, 0), axis=0)\n",
    "    delta = out_xy - t_[:, d]\n",
    "    bary = np.einsum('njk,nk->nj', t_[:, :d, :], delta)\n",
    "    wgts = np.hstack((bary, 1 - bary.sum(axis=1, keepdims=True)))\n",
    "    # Points outside the out_xy\n",
    "    out_idx_out = s < 0  \n",
    "    if np.any(out_idx_out):\n",
    "        # For points outside, find nearest neighbors\n",
    "        tree = cKDTree(in_xy)\n",
    "        _, in_idx_out = tree.query(out_xy[out_idx_out])\n",
    "    else : \n",
    "        in_idx_out = None\n",
    "    return vert, wgts, out_idx_out, in_idx_out\n",
    "\n",
    "\n",
    "def interp(values, vtx, wts, out_idx_out, in_idx_out):\n",
    "    res = np.einsum('nj,nj->n', np.take(values, vtx), wts)\n",
    "    if in_idx_out is not None:\n",
    "        res[out_idx_out] = values[in_idx_out]\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "here we will mimic the first function from above in order to append to a zarr store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Function to subset ERA data based on the mesh extent\n",
    "def subset_era_from_mesh(\n",
    "    era: xr.Dataset, \n",
    "    mesh: xr.Dataset, \n",
    "    input360: bool,\n",
    "    gtype:str,\n",
    ") -> xr.Dataset:\n",
    "    \"\"\"\n",
    "    Selects a subset of ERA data that overlaps with the provided mesh's geographical extent.\n",
    "\n",
    "    :param era: The ERA dataset from which to select a subset.\n",
    "    :param mesh: The mesh dataset defining the geographical extent for the subset selection.\n",
    "    :param input360: A flag indicating whether the longitude should be adjusted to a 0-360 range.\n",
    "    :return: A subset of the ERA dataset that falls within the mesh's geographical extent.\n",
    "    \"\"\"\n",
    "    xmin, xmax, ymin, ymax = mesh.lon.min(), mesh.lon.max(), mesh.lat.min(), mesh.lat.max()\n",
    "    if input360:\n",
    "        xmin, xmax = np.mod(xmin+360, 360), np.mod(xmax+360, 360)\n",
    "        if xmax < xmin:\n",
    "            xmin, xmax = 0, 360\n",
    "    if gtype == \"grid\":\n",
    "        era_chunk = era.sel(longitude=slice(xmin, xmax), latitude=slice(ymax, ymin))\n",
    "    else: # for 01280 grid\n",
    "        mask_lon = (era.longitude >= xmin) & (era.longitude <= xmax)\n",
    "        mask_lat = (era.latitude >= ymin) & (era.latitude <= ymax)\n",
    "        mask = mask_lon & mask_lat\n",
    "        indices = np.where(mask)[0]\n",
    "        era_chunk = era.isel(values=indices)\n",
    "    return era_chunk\n",
    "\n",
    "# Function to write meteorological data onto a mesh\n",
    "def write_meteo_on_mesh(\n",
    "    era_ds: xr.Dataset, \n",
    "    mesh: xr.Dataset, \n",
    "    file_out: str, \n",
    "    n_time_chunk: int, \n",
    "    n_node_chunk: int, \n",
    "    input360: bool = True,\n",
    "    gtype: str = \"grid\",\n",
    "    ttype: str = \"time\"\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Writes meteorological data from an ERA dataset onto a mesh and saves the result as a zarr file.\n",
    "\n",
    "    :param era_ds: The ERA dataset with the meteorological data.\n",
    "    :param mesh: The mesh dataset representing the spatial domain.\n",
    "    :param file_out: The path to the output zarr file.\n",
    "    :param n_time_chunk: The size of the time chunks for processing.\n",
    "    :param n_node_chunk: The size of the node chunks for processing.\n",
    "    :param input360: A flag indicating whether the longitude should be adjusted to a 0-360 range.\n",
    "    \"\"\"\n",
    "    # Create the temporary dummy zarr file\n",
    "    if os.path.exists(file_out):\n",
    "        shutil.rmtree(file_out)\n",
    "    x, y, tri = mesh.lon.values, mesh.lat.values, mesh.triface_nodes.values\n",
    "    nnodes = len(x)\n",
    "    ntimes = len(era_ds.time)\n",
    "    zero = da.zeros((ntimes, nnodes), chunks=(n_time_chunk, n_node_chunk))\n",
    "    \n",
    "    # Define coordinates and data variables for the output dataset\n",
    "    coords = {\n",
    "        'time': era_ds.time, \n",
    "        'node': np.arange(nnodes),\n",
    "        'lon': (\"node\", x), \n",
    "        'lat': (\"node\", y),\n",
    "        'triface_nodes': mesh.triface_nodes,\n",
    "    }\n",
    "    data_vars = {}\n",
    "    for varin in era_ds.data_vars:\n",
    "        data_vars[varin] = (('time', 'node'), zero)\n",
    "    xr.Dataset(data_vars=data_vars, coords=coords).to_zarr(file_out, compute=False)\n",
    "    \n",
    "    # in the case of \"tstps\"\n",
    "    if ttype == \"step\":\n",
    "        t0 = pd.Timestamp(era_ds.time.values)\n",
    "        seconds = era_ds.step.values / 1e9\n",
    "        era_ds.time = pd.to_datetime(t0 + pd.Timedelta(seconds=seconds))\n",
    "\n",
    "    # Iterate over nodes in chunks and write data to zarr file\n",
    "    for ins in range(0, nnodes, n_node_chunk):\n",
    "        end_node = min(ins + n_node_chunk, nnodes)\n",
    "        node_chunk = np.arange(ins, end_node)\n",
    "        mesh_chunk = mesh.isel(node=slice(ins, end_node))\n",
    "        era_chunk = subset_era_from_mesh(era_ds, mesh_chunk, input360=input360, gtype=gtype)\n",
    "        \n",
    "        # Get weights for interpolation\n",
    "        if gtype == \"grid\":\n",
    "            nx1d = len(era_chunk.longitude)\n",
    "            ny1d = len(era_chunk.latitude)\n",
    "            xx = np.tile(era_chunk.longitude, ny1d).reshape(ny1d, nx1d).T.ravel()\n",
    "            yy = np.tile(era_chunk.latitude, nx1d)\n",
    "        else: # for O1280 grids\n",
    "            xx = era_chunk.longitude\n",
    "            yy = era_chunk.latitude\n",
    "            era_chunk = era_chunk.drop_vars([\"number\", \"surface\"]) # useless for meteo exports\n",
    "        \n",
    "        in_xy = np.vstack((xx, yy)).T\n",
    "        if input360:\n",
    "            in_xy[:, 0][in_xy[:, 0] > 180] -= 360\n",
    "        out_xy = np.vstack((mesh_chunk.lon, mesh_chunk.lat)).T\n",
    "        vert, wgts, u_x, g_x = get_weights(in_xy, out_xy)  # Assuming get_weights is defined elsewhere\n",
    "        \n",
    "        # Interpolate and write data for each variable and time chunk\n",
    "        for var_name in era_chunk.data_vars:\n",
    "            for it_chunk in range(0, ntimes, n_time_chunk):\n",
    "                t_end = min(it_chunk + n_time_chunk, ntimes)\n",
    "                time_chunk = era_chunk.time[it_chunk:t_end]\n",
    "                data_chunk = da.zeros((len(time_chunk), len(node_chunk)), chunks=(n_time_chunk, n_node_chunk))\n",
    "                for it, t_ in enumerate(time_chunk):\n",
    "                    tmp = np.ravel(np.transpose(era_chunk.isel(time=it_chunk+it)[var_name].values))\n",
    "                    data_chunk[it,:] = interp(tmp, vert, wgts, u_x, g_x)  # Assuming interp is defined elsewhere\n",
    "                coords = {'time':time_chunk, 'node': node_chunk}\n",
    "                ds = xr.Dataset({var_name: (('time', 'node'), data_chunk)}, coords=coords)\n",
    "                region = {'time': slice(it_chunk, t_end), 'node': slice(ins, end_node)}\n",
    "                ds.to_zarr(file_out, mode=\"a-\", region=region)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the script is ready, let's do a sensitivity on the nodes & times "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let's reduce first the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "era_test = era5.isel(time=slice(0,1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "times_ = [50,]\n",
    "nodes_ = [len(mesh_out.lon)]\n",
    "\n",
    "time_perf = np.zeros((len(times_), len(nodes_)))\n",
    "sizes = np.zeros((len(times_), len(nodes_)))\n",
    "for i_t, ttime in enumerate(times_): \n",
    "    for i_n, nnode in enumerate(nodes_):\n",
    "        start = time.time()\n",
    "        write_meteo_on_mesh(era_test, mesh_out, \"era5_2023_uvp.zarr\", ttime, nnode)\n",
    "        end = time.time()\n",
    "        time_perf[i_t,i_n] =  end - start\n",
    "        # calculate size\n",
    "        bytes_per_element = zero.dtype.itemsize\n",
    "        sizes[i_t,i_n] = ttime * nnode * bytes_per_element  / 1024 / 1000\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "visualise the performances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert data for pcolormesh plots into a format compatible with hvplot\n",
    "time_perf_mesh_df = pd.DataFrame(time_perf, index=times_, columns=nodes_)\n",
    "sizes_mesh_df = pd.DataFrame(sizes, index=times_, columns=nodes_)\n",
    "\n",
    "options = {\n",
    "    \"x\":'columns', \n",
    "    \"y\":'index', \n",
    "    \"C\":'value', \n",
    "    \"logx\" : True, \n",
    "    \"logy\" : True, \n",
    "    \"colorbar\" : True, \n",
    "    \"xlabel\" : \"chunk size for nodes\", \n",
    "    \"ylabel\" :  \"chunk size for times\", \n",
    "    \"width\" :  500,\n",
    "}\n",
    "\n",
    "# Convert data for line plot into a Pandas DataFrame\n",
    "sizes_flat = sizes.ravel()\n",
    "time_perf_flat = time_perf.ravel()\n",
    "idx = np.argsort(sizes_flat)\n",
    "line_df = pd.DataFrame({\n",
    "    'Size (MB)': sizes_flat[idx],\n",
    "    'Time (sec)': time_perf_flat[idx]\n",
    "})\n",
    "\n",
    "# Create hvplot pcolormesh plots\n",
    "time_perf_plot = time_perf_mesh_df.hvplot.heatmap(**options).opts(title = \"Time taken for each chunk (in seconds)\")\n",
    "sizes_plot = sizes_mesh_df.hvplot.heatmap(**options).opts(title = \"Size of each chunk (in MB)\")\n",
    "\n",
    "# Create hvplot line plot\n",
    "line_plot = line_df.hvplot.line(\n",
    "    x='Size (MB)', y='Time (sec)', \n",
    "    title='Time taken vs. Size of each chunk', width = 500)\n",
    "\n",
    "# Combine plots into a layout\n",
    "layout = (time_perf_plot + sizes_plot + line_plot).cols(3)\n",
    "\n",
    "# Apply some options for better layout\n",
    "layout.opts(\n",
    "    opts.HeatMap(colorbar=True),\n",
    "    opts.Layout(shared_axes=False, merge_tools=False),\n",
    ")\n",
    "# # Display the layout\n",
    "hv.output(layout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best combination seems to be the **max number of nodes with the minimum number of times**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "check if it worked: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = xr.open_dataset(\"era5_2023_uvp.zarr\", engine = \"zarr\")\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import thalassa\n",
    "import geoviews as gv\n",
    "ds = thalassa.normalize(ds)\n",
    "# Convert the node data in \"Tabular\" format: https://holoviews.org/user_guide/Tabular_Datasets.html\n",
    "df = ds[[\"lon\", \"lat\", \"node\"]].to_dataframe().reset_index()\n",
    "# Create a geoviews Points object\n",
    "gv_points = gv.Points(df, kdims=[\"lon\", \"lat\"], vdims=[\"node\"]).opts(size = 1, color='k')\n",
    "# Create a Dynamic map with the variable you want, e.g. \"depth\"\n",
    "raster_dmap = thalassa.plot(ds.sel(time = \"2023-08-27\", method=\"nearest\"), \"u10\")\n",
    "# now combine the raster with the points\n",
    "combined_dmap = raster_dmap * gv_points\n",
    "# plot\n",
    "combined_dmap.opts(tools=[\"hover\"], width=1200, height=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### finally, convert to Selafin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove(path):\n",
    "    try:\n",
    "        if os.path.isfile(path):\n",
    "            os.remove(path)  # Remove a file\n",
    "        elif os.path.isdir(path):\n",
    "            if not os.listdir(path):  # Check if the directory is empty\n",
    "                os.rmdir(path)\n",
    "            else:\n",
    "                shutil.rmtree(path)\n",
    "    except OSError as e:\n",
    "        print(f\"Error: {e.strerror}\")\n",
    "        \n",
    "def write_meteo_selafin(outpath,input_atm_zarr):\n",
    "    xatm = xr.open_dataset(input_atm_zarr, engine = \"zarr\")  \n",
    "    t0 = pd.Timestamp(ds.time.values[0])\n",
    "    # Define a mapping from the original variable names to the new ones\n",
    "    var_map = {\n",
    "        \"u10\": (\"WINDX\", \"M/S\"),\n",
    "        \"v10\": (\"WINDY\", \"M/S\"),\n",
    "        \"msl\": (\"PATM\", \"PASCAL\"),\n",
    "        \"tmp\": (\"TAIR\", \"DEGREES C\"),\n",
    "    }\n",
    "    var_attrs = {}\n",
    "    for var in ds.data_vars:\n",
    "        if var in var_map:\n",
    "            # Attributes for the variable\n",
    "            var_attrs[var] = (var_map[var][0], var_map[var][1])\n",
    "    # Add global attributes after concatenation\n",
    "    xatm.attrs[\"date_start\"] = [t0.year, t0.month, t0.day, t0.hour, t0.minute, t0.second]\n",
    "    xatm.attrs[\"ikle2\"] = xatm.triface_nodes.values + 1\n",
    "    xatm.attrs[\"variables\"] = {var: attrs for var, attrs in var_attrs.items()}\n",
    "    xatm = xatm.rename({\"lon\": \"x\", \"lat\": \"y\"})\n",
    "    xatm = xatm.drop_vars([\"triface_nodes\"])\n",
    "    xatm.selafin.write(outpath)\n",
    "    # remove(input_atm_zarr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_meteo_selafin(\"era5_2023_uvp.slf\", \"era5_2023_uvp.zarr\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test with O1280 grid: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "O1280_test = xr.open_dataset(\"2023-09-01.uvp.O1280.zarr\", engine = \"zarr\")\n",
    "O1280_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nnode = len(O1280_test.longitude)\n",
    "write_meteo_on_mesh(O1280_test, mesh_out, \"01280_v0.zarr\", 50, nnode, input360=True, gtype='tri')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_meteo_selafin(\"01280_202309_uvp.slf\", \"01280_v0.zarr\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "it works !! (GIF you see at the beginning of the notebook)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
