{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains the methodology for: \n",
    " 1. choosing the best IOC candidates for storm surge validation purposes\n",
    " 2. extracting STOFS2D data\n",
    "\n",
    "and edit of this notebook will be done for: \n",
    "\n",
    " 3. the data availability\n",
    " 4. the data quality \n",
    "of IOC candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gp\n",
    "import pandas as pd\n",
    "import searvey\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import sklearn.neighbors\n",
    "import xarray as xr\n",
    "import hvplot.pandas\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will extract observed data from the STOF2D model, we will focus on the points locations exported by the model. \n",
    "\n",
    "The files have the following format: `stofs_2d_glo.tCCz.points.{cwl,htp,swl}.nc` for \"Six-minute station time series water level (m, MSL) forecast files at verification sites\"\n",
    " * `swl` is storm surge only\n",
    " * `htp` is astromical tide only\n",
    " * `cwl` is combined water level (`swl` + `htp`)\n",
    "\n",
    "More information can be found on this page: https://noaa-gestofs-pds.s3.amazonaws.com/README.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A file available at the following address references all stofs 2D stations:\n",
    "\n",
    "def get_stofs():\n",
    "    mycols = [str(i) for i in range(6)] # we expect 17 cols max in that file\n",
    "    stof2d = pd.read_csv(\n",
    "        \"https://polar.ncep.noaa.gov/stofs/data/stofs_2d_glo_elev_stat_v2_1_0\",\n",
    "        names=mycols, \n",
    "        sep=\"\\t+|!\", \n",
    "        header=None, \n",
    "        skiprows=1\n",
    "    )\n",
    "    stof2d['Info'] = stof2d.apply(lambda row: ' '.join(filter(None, row[2:])), axis=1)\n",
    "    stof2d['ID'] = stof2d['Info'].apply(lambda x: ' '.join(x.split()[:3]))\n",
    "    stof2d['Info'] = stof2d.apply(lambda row: row['Info'].replace(row['ID'], '').strip(), axis=1)\n",
    "    stof2d = stof2d.drop(columns=[\"2\", \"3\", \"4\", \"5\"])\n",
    "    stof2d.rename(columns={\"0\": 'lon', \"1\": 'lat'}, inplace=True)\n",
    "    return stof2d\n",
    "\n",
    "stofs = get_stofs()\n",
    "stofs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A caveat is that the 1D output files evolve over time: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stofs1 = xr.open_dataset(\"stofs2d/20220912_stofs_2d_glo.t18z.points.swl.nc\")\n",
    "stofs2 = xr.open_dataset(\"stofs2d/20231010_stofs_2d_glo.t18z.points.swl.nc\")\n",
    "stofs3 = xr.open_dataset(\"stofs2d/20241229_stofs_2d_glo.t12z.points.swl.nc\")\n",
    "\n",
    "stofs_2022 = stofs[stofs.ID.isin([' '.join(s.decode(\"utf-8\").strip().split()[:3]) for s in stofs1.station_name.values])];len(stofs_2022)\n",
    "stofs_2023 = stofs[stofs.ID.isin([' '.join(s.decode(\"utf-8\").strip().split()[:3]) for s in stofs2.station_name.values])];len(stofs_2023)\n",
    "stofs_2024 = stofs[stofs.ID.isin([' '.join(s.decode(\"utf-8\").strip().split()[:3]) for s in stofs3.station_name.values])];len(stofs_2024)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "luckily the new stations were appended at the end of the file. So this will be easier to concatenate data between all the files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stofs_2022[:557].equals(stofs_2023[:557])\n",
    "stofs_2022[:557].equals(stofs_2024[:557])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to compare model storm surge with observation. We use IOC tide stations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_meta() -> gp.GeoDataFrame:\n",
    "    meta_web = searvey.get_ioc_stations().drop(columns=[\"lon\", \"lat\"])\n",
    "    meta_api = (\n",
    "        pd.read_json(\n",
    "            \"http://www.ioc-sealevelmonitoring.org/service.php?query=stationlist&showall=all\"\n",
    "        )\n",
    "        .drop_duplicates()\n",
    "        .drop(columns=[\"lon\", \"lat\"])\n",
    "        .rename(columns={\"Code\": \"ioc_code\", \"Lon\": \"lon\", \"Lat\": \"lat\"})\n",
    "    )\n",
    "    merged = pd.merge(\n",
    "        meta_web,\n",
    "        meta_api[[\"ioc_code\", \"lon\", \"lat\"]].drop_duplicates(),\n",
    "        on=[\"ioc_code\"],\n",
    "    )\n",
    "    return merged.drop(columns=[\"geometry\"])\n",
    "ioc_ = get_meta()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We already have established a database for clean IOC data between 2022 and 2023, we'll use it as a reference: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "IOC_CLEANUP = \"ioc_cleanup_2023.csv\"\n",
    "ioc_cleanup = pd.read_csv(IOC_CLEANUP, index_col=0).rename(columns={\"longitude\": 'lon', \"latitude\": 'lat', \"Station_Name\":\"location\",\"Country\":\"country\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stofs_plot = stofs_2022.hvplot.scatter(x= \"lon\", y=\"lat\", hover_cols = \"ID\", s=130, c='lightgrey', label = 'STOFS 2022 output stations')\n",
    "stofs_plot1 = stofs_2023.hvplot.scatter(x=\"lon\", y=\"lat\", hover_cols = \"ID\", s=150, c='grey', label = 'STOFS 2023 output stations')\n",
    "stofs_plot2 = stofs_2024.hvplot.scatter(x=\"lon\", y=\"lat\", hover_cols = \"ID\", s=200, c='k', label = 'STOFS 2024 output stations')\n",
    "ioc_plot = ioc_.hvplot.scatter(x=\"lon\", y=\"lat\",hover_cols = \"ioc_code\", s= 30 , c = 'y', label = 'all IOC stations')\n",
    "ioc_cleanup_plot = ioc_cleanup.hvplot.scatter(x=\"lon\", y=\"lat\",s = 80, c='r', label = \"stations cleaned for 2022-2023\")\n",
    "\n",
    "(stofs_plot2 * stofs_plot1 * stofs_plot * ioc_cleanup_plot* ioc_plot).opts(width = 1600, height = 800)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We graphically detected all stations not already used in `ioc_cleanup` and corresponding with STOFS2D output locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "station_to_add = [\"juan\", \"sanf\", \"anto\", \"ptmo\", \"valp\", \"ferg\", \"ambon\", \"bitu\", \"saum\", \"sho2\", \"ushu\", \n",
    "                  \"espr\", \"gamb\", \"riki\", \"prud\", \"vald\", \"cord\", \"paak\", \"dsea\", \"ketc\", \"june\", \"skag\", \"sewa\", \"anch\", \"niki\", \"seld\", \"kodi\", \"alak\", \n",
    "                  \"dshu\", \"dkod\", \"nome\", \"adak\", \"niko\", \"dchu\", \"midx\", \"fren\", \"sthl\", \"ascen\", \"jask\", \"chab\", \"kara\", \"musc\", \n",
    "                  \"masi\", \"mais\", \"kerg\", \"syow\", \"ver1\", \"vern\", \"wait\", \"stpa\", \"sala\", \"tara\", \"marsh\", \"kwaj\", \"wake\", \"fong\", \n",
    "                  \"solo\", \"vanu\", \"numbo\", \"numb2\", \"levu\", \"wlgt\", \"jack\", \"hako\", \"abas\", \"ofun\", \"mera\", \"toya\", \"nawi\", \"brpt\", \"heeia\", \n",
    "                  \"moku\", \"mane\", \"john\", \"plmy\", \"xmas\", \"penr\", \"hiva\", \"pape\", \"raro\", \"pago\", \"pagx\", \"east\", \"garc\", \"Male2\", \"ganm\", \"male\", \"hani\", \n",
    "                  \"mini\", \"coch\", \"vish\", \"chtt\", \"sitt\", \"moul\", \"ptbl\", \"komi\", \"kota\", \"lank\", \"ms001\", \"sab2\", \"saba\", \"vung\", \"quin\", \n",
    "                  \"quar\", \"curri\", \"subi\", \"mani\", \"luba\", \"lega\", \"tkao\", \"tkee\", \"chij\", \"mins\", \"saip\", \"mala\", \"chuu\", \"kapi\", \"deke\", \"naur\", \"nauu\", \n",
    "                  \"dumo\", \"espe\", \"porl\", \"hill\", \"waik\", \"lemba\", \"beno\", \"prgi\", \"prig\", \"cili\", \"cila\", \"tjls\", \"chrs\", \"ffcj\", \"cocb\", \"telu\", \"sibo\", \n",
    "                  \"sib2\", \"tanjo\", \"bupo\", \"padn\", \"pada\", \"fpga\", \"winc\", \"wbnc\", \"oinc\", \"kpva\", \"leva\", \"simd\", \"wsdc\", \"cbmd\", \"ocmd\", \"cmnj\", \"phap\", \n",
    "                  \"mhpa\", \"btny\", \"shnj\", \"mony\", \"ptme\", \"cwme\", \"epme\", \"hali\", \"nain\", \"nuk1\", \"nuuk\", \"qaqo\", \"reyk\", \"scor\", \"rptx\", \"cctx\", \"pitx\", \n",
    "                  \"pric\", \"ftfr\", \"rose\", \"barb\", \"stcr\", \"lame\", \"isab\", \"vieq\", \"yobu\", \"yabu\", \"faja\", \"sanj\", \"arac\", \"maya\", \"magi\", \"penu\", \"mona\", \n",
    "                  \"ptpr\", \"ptpl\", \"sama\", \"bull\", \"elpo\", \"limon\", \"quepo\", \"sana\", \"acaj\", \"acap\", \"acya\", \"manz\", \"mnza\", \"cabo\", \"fort\", \"call\", \"lobos\", \n",
    "                  \"tala\", \"lali\", \"vkfl\", \"nafl\", \"fmfl\", \"spfl\", \"pnfl\", \"pbfl\", \"apfl\", \"tpfl\", \"fbfl\", \"moal\", \"wlms\", \"psla\", \"gila\", \"pfla\", \"ncla\", \n",
    "                  \"apla\", \"eila\", \"cpla\", \"sptx\", \"gptx\", \"fptx\", \"bres\", \"sthm\", \"casc\", \"gibr\", \"ceut\", \"mars\", \"TR22\", \"gvd9\", \"alex\", \"palm\", \"pdas\", \n",
    "                  \"plus\", \"dakar\", \"tako\", \"tkdi\", \"lagos\", \"pntn\", \"sitin\", \"walvi\", \"prte\", \"durb\", \"pemba\", \"mtwa\", \"momb\", \"lamu\", \"pmon\", \"aric\", \"mata\", \n",
    "                  \"plat\", \"salv\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "some station can be declined in different names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "possible_stations = []\n",
    "all_ioc = ioc_.ioc_code.values\n",
    "for stat in station_to_add:\n",
    "    if any(stat in station for station in all_ioc):\n",
    "        for station in all_ioc:\n",
    "            if stat in station:\n",
    "                possible_stations.append(station)\n",
    "ioc_to_add = ioc_[ioc_.ioc_code.isin(possible_stations)]\n",
    "ioc_to_add"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stofs_plot =  stofs_2022.hvplot.scatter(x= \"lon\", y=\"lat\", hover_cols = \"ID\", s=130, c='lightgrey', label = 'STOFS 2022 output stations')\n",
    "stofs_plot1 = stofs_2023.hvplot.scatter(x=\"lon\", y=\"lat\", hover_cols = \"ID\", s=150, c='grey', label = 'STOFS 2023 output stations')\n",
    "stofs_plot2 = stofs_2024.hvplot.scatter(x=\"lon\", y=\"lat\", hover_cols = \"ID\", s=200, c='k', label = 'STOFS 2024 output stations')\n",
    "ioc_plot = ioc_.hvplot.scatter(x=\"lon\", y=\"lat\",hover_cols = \"ioc_code\", s= 30 , c = 'y',label = 'all IOC stations')\n",
    "ioc_cleanup_plot = ioc_cleanup.hvplot.scatter(x=\"lon\", y=\"lat\",s = 90, c='r',label = 'stations already cleaned for 2022-2023')\n",
    "ioc_to_add_plot = ioc_to_add.hvplot.scatter(x=\"lon\", y=\"lat\",s = 90, geo=True, c = 'g', label = 'stations to be added')\n",
    "\n",
    "(stofs_plot2 * stofs_plot1 * stofs_plot * ioc_to_add_plot * ioc_cleanup_plot).opts(width = 1800, height = 800)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the 2024 IOC cleanup database is the red + green points "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ioc_cleanup_2024 = pd.concat([ioc_cleanup,ioc_to_add])\n",
    "ioc_cleanup_2024\n",
    "ioc_cleanup_2024.to_csv(\"ioc_cleanup_2024.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_nearest_nodes(\n",
    "    mesh_nodes: pd.DataFrame,\n",
    "    points: pd.DataFrame,\n",
    "    metric: str = \"haversine\",\n",
    "    earth_radius = 6371000,\n",
    "    ):\n",
    "    \"\"\"\n",
    "    Calculate the mesh nodes that are nearest to the specified `points`.\n",
    "    Both `mesh_nodes` and `points` must be `pandas.DataFrames` that have\n",
    "    columns named `lon` and `lat` and the coords must be in EPSG:4326.\n",
    "    Returns the `points` DataFrame after adding these extra columns:\n",
    "    - `mesh_index` which is the index of the node in the `hgrid.gr3` file\n",
    "    - `mesh_lon` which is the longitude of the nearest mesh node\n",
    "    - `mesh_lat` which is the latitude of the nearest mesh node\n",
    "    - `distance` which is the distance in meters between the point and the nearest mesh node\n",
    "    Examples:\n",
    "        >>> mesh_nodes = pd.DataFrame({\n",
    "        ...     \"lon\": [0, 10, 20],\n",
    "        ...     \"lat\": [0, 5, 0],\n",
    "        ... })\n",
    "        >>> points = pd.DataFrame({\n",
    "        ...     \"lon\": [1, 11, 21],\n",
    "        ...     \"lat\": [1, 4, 1],\n",
    "        ...     \"id\": [\"a\", \"b\", \"c\"],\n",
    "        ... })\n",
    "        >>> nearest_nodes = find_nearest_nodes(mesh_nodes, points)\n",
    "        >>> nearest_nodes\n",
    "           lon  lat id  mesh_index  mesh_lon  mesh_lat       distance\n",
    "        0    1    1  a           0         0         0  157249.381272\n",
    "        1   11    4  b           1        10         5  157010.162641\n",
    "        2   21    1  c           2        20         0  157249.381272\n",
    "    \"\"\"\n",
    "    # The only requirement is that both `mesh_nodes and `points` have `lon/lat` columns\n",
    "    tree = sklearn.neighbors.BallTree(\n",
    "        np.radians(mesh_nodes[[\"lat\", \"lon\"]]),\n",
    "        metric=metric,\n",
    "    )\n",
    "    distances, indices = tree.query(np.radians(points[[\"lat\", \"lon\"]].values))\n",
    "    closest_nodes = (\n",
    "        mesh_nodes\n",
    "        .rename(columns={\"lon\": \"mesh_lon\", \"lat\": \"mesh_lat\"})\n",
    "        .iloc[indices.flatten()]\n",
    "        .assign(distance=(distances.flatten() * earth_radius))\n",
    "        .reset_index(names=[\"mesh_index\"])\n",
    "    )\n",
    "\n",
    "    return pd.concat((points.reset_index(drop = True), closest_nodes), axis=\"columns\")\n",
    "\n",
    "# 2 - get STOFS\n",
    "nearest_nodes_2022 = find_nearest_nodes(stofs_2022, ioc_cleanup_2024[[\"lon\",\"lat\",\"ioc_code\",\"location\"]])\n",
    "nearest_nodes_2023 = find_nearest_nodes(stofs_2023, ioc_cleanup_2024[[\"lon\",\"lat\",\"ioc_code\",\"location\"]])\n",
    "nearest_nodes_2024 = find_nearest_nodes(stofs_2024, ioc_cleanup_2024[[\"lon\",\"lat\",\"ioc_code\",\"location\"]])\n",
    "nearest_nodes_2022 = nearest_nodes_2022[~nearest_nodes_2022.mesh_index.isna()]\n",
    "nearest_nodes_2023 = nearest_nodes_2023[~nearest_nodes_2023.mesh_index.isna()]\n",
    "nearest_nodes_2024 = nearest_nodes_2024[~nearest_nodes_2024.mesh_index.isna()]\n",
    "keep_nodes_2022 = nearest_nodes_2022[nearest_nodes_2022.distance < 5000]\n",
    "keep_nodes_2023 = nearest_nodes_2023[nearest_nodes_2023.distance < 5000]\n",
    "keep_nodes_2024 = nearest_nodes_2024[nearest_nodes_2024.distance < 5000]\n",
    "\n",
    "keep_nodes_2022.to_csv(\"keep_nodes_2022.csv\")\n",
    "keep_nodes_2023.to_csv(\"keep_nodes_2023.csv\")\n",
    "keep_nodes_2024.to_csv(\"keep_nodes_2024.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "red are all the STOFS2D points to be extracted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p2 = stofs_2022.hvplot.scatter(x=\"lon\", y=\"lat\", hover_cols = \"ID\", s=50, c='grey', label = 'STOFS 2022 output stations')\n",
    "ip = ioc_cleanup_2024.hvplot.scatter(x=\"lon\", y=\"lat\",s = 10, c='g',label = 'IOC_CLEANUP 2022-2024')\n",
    "k2 = keep_nodes_2022.hvplot.scatter(x=\"lon\", y=\"lat\", c = 'red',    s = 10, label = \"STOFS2D stations to be extracted\")\n",
    "\n",
    "(p2 * ip * k2).opts(width = 1800, height = 800)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "download STOFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "\n",
    "import pandas as pd\n",
    "import httpx\n",
    "import multifutures\n",
    "\n",
    "base_path = \"stofs2d\"\n",
    "\n",
    "pathlib.Path(base_path).mkdir(exist_ok=True)\n",
    "\n",
    "# The URL changed at 2023-01-08 \n",
    "# These are the new urls \n",
    "base_url = \"https://noaa-nos-stofs2d-pds.s3.amazonaws.com/stofs_2d_glo.{date_str}/stofs_2d_glo.t{time}z.points.swl.nc\"\n",
    "new_url_names = {}\n",
    "for date in pd.date_range(\"2023-01-08\", \"2024-12-31\"):\n",
    "    date_str = date.strftime(\"%Y%m%d\")\n",
    "    for time in (\"00\", \"06\", \"12\", \"18\"):\n",
    "        url = base_url.format(date_str=date_str, time=time)\n",
    "        name = f\"{date_str}_{url.rsplit('/', 1)[1]}\"\n",
    "        new_url_names[url] = name\n",
    "\n",
    "# These are the old urls \n",
    "base_url = \"https://noaa-nos-stofs2d-pds.s3.amazonaws.com/estofs.{date_str}/estofs.t{time}z.points.swl.nc\"\n",
    "old_url_names = {}\n",
    "for date in pd.date_range(\"2022-01-01\", \"2023-01-07\"):\n",
    "    date_str = date.strftime(\"%Y%m%d\")\n",
    "    for time in (\"00\", \"06\", \"12\", \"18\"):\n",
    "        url = base_url.format(date_str=date_str, time=time)\n",
    "        name = f\"{date_str}_{url.rsplit('/', 1)[1].replace('estofs', 'stofs_2d_glo')}\"\n",
    "        old_url_names[url] = name\n",
    "\n",
    "def download_file(client, url, name):\n",
    "    with open(name, \"wb\") as fd:\n",
    "        with client.stream(\"GET\", url) as response:\n",
    "            for data in response.iter_bytes():\n",
    "                fd.write(data)\n",
    "\n",
    "with httpx.Client() as client:\n",
    "    func_kwargs = [{\"client\": client, \"url\": url, \"name\": f\"{base_path}/{name}\"} for (url, name) in new_url_names.items()]\n",
    "    multifutures.multithread(func=download_file, func_kwargs=func_kwargs, check=True)\n",
    "\n",
    "with httpx.Client() as client:\n",
    "    func_kwargs = [{\"client\": client, \"url\": url, \"name\": f\"{base_path}/{name}\"} for (url, name) in old_url_names.items()]\n",
    "    multifutures.multithread(func=download_file, func_kwargs=func_kwargs, check=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "STOFS_FOLDER = \"stofs2d/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "extract stations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "# gather stofs2D data\n",
    "ds1 = []\n",
    "ds2 = []\n",
    "ds3 = []\n",
    "for file in sorted(glob.glob(STOFS_FOLDER + \"*.swl.nc\")):\n",
    "    root, filename = os.path.split(file)\n",
    "    date = datetime.strptime(filename, \"%Y%m%d_stofs_2d_glo.t%Hz.points.swl.nc\")\n",
    "    tmp = xr.open_dataset(file)\n",
    "    \n",
    "    if date > datetime(2024,5,14,11):\n",
    "        ds3.append(tmp.sel(time=slice(date,date+pd.Timedelta(hours=6))))\n",
    "        print(date, len(tmp.station_name),\"ds3\")\n",
    "    else:\n",
    "        if date > datetime(2023,1,7, 23):\n",
    "            ds2.append(tmp.sel(time=slice(date,date+pd.Timedelta(hours=6))))\n",
    "            print(date, len(tmp.station_name),\"ds2\")\n",
    "        else: # date < datetime.datetime(2023,1,7)\n",
    "            ds1.append(tmp.sel(time=slice(date,date+pd.Timedelta(hours=6))))\n",
    "            print(date, len(tmp.station_name),\"ds1\")\n",
    "# keep fist time \n",
    "ds1 = xr.concat(ds1, dim=\"time\")\n",
    "ds2 = xr.concat(ds2, dim=\"time\")\n",
    "ds3 = xr.concat(ds3, dim=\"time\")\n",
    "for it, station in keep_nodes_2022.iterrows(): # take 2022 because it has less nodes\n",
    "    print(f'Station {station.ioc_code} at {station.lon:.2f}, {station.lat:.2f}')\n",
    "    ds1_subset = ds1.isel(station = int(station.mesh_index))\n",
    "    ds2_subset = ds2.isel(station = int(station.mesh_index))\n",
    "    ds3_subset = ds3.isel(station = int(station.mesh_index))\n",
    "    ds_subset = xr.concat([ds1_subset, ds2_subset, ds3_subset], dim=\"time\")\n",
    "    df = ds_subset.to_pandas()\n",
    "    df.to_parquet(f'data/{station.ioc_code}.parquet')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "searvey",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
